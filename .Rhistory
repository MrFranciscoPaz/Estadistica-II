salida_grad <- function(beta){
p_beta <- h(as.matrix(cbind(1, x_ent)) %*% beta)
e <- y_ent - p_beta
grad_out <- -2*as.numeric(t(cbind(1,x_ent)) %*% e)
names(grad_out) <- c('Intercept', colnames(x_ent))
grad_out
}
salida_grad
}
=======
rss_calc <- function(datos){
# esta función recibe los datos (x,y) y devuelve
# una función f(betas) que calcula rss
y <- datos$lpsa
x <- datos$lcavol
fun_out <- function(beta){
y_hat <- beta[1] + beta[2]*x
e <- (y - y_hat)
rss <- sum(e^2)
0.5*rss
}
fun_out
}
prostata <- read_csv('datos/prostate.csv') %>% select(lcavol, lpsa, train)
kable(head(prostata), digits = 4)
prostata_entrena <- filter(prostata, train)
ggplot(prostata_entrena, aes(x = lcavol, y = lpsa)) + geom_point() + theme_bw()
rss_prostata <- rss_calc(prostata_entrena)
beta <- c(0,1.5)
rss_prostata(beta)
ggplot(prostata_entrena, aes(x = lcavol, y = lpsa)) + geom_point() +
geom_abline(slope = beta[2], intercept = beta[1], col ='red') + theme_bw()
beta <- c(1,1)
rss_prostata(beta)
ggplot(prostata_entrena, aes(x = lcavol, y = lpsa)) + geom_point() +
geom_abline(slope = beta[2], intercept = beta[1], col ='red') + theme_bw()
grad_prostata <- grad_calc(prostata_entrena[, 1, drop = FALSE], prostata_entrena$lpsa)
grad_prostata(c(0,1))
grad_prostata(c(1,1))
iteraciones <- descenso(100, c(0,0), 0.005, grad_prostata)
iteraciones
iteraciones[100,1]
(modelo <- lm(prostata_entrena$lpsa ~ prostata_entrena$lcavol))
summary(modelo)
ggplot(prostata_entrena, aes(x = lcavol, y = lpsa)) + geom_point() +
geom_abline(slope = iteraciones[100,2] , intercept =iteraciones[100,1], col ='red') +
geom_abline(slope = modelo$coefficients[2], intercept = modelo$coefficients[1]
, col ='blue',linetype="dashed")+
theme_bw()
plot(cars$dist,cars$speed)
cars <- cars
kable(head(cars))
modelo <-  lm(cars$dist ~ cars$speed)
modelo
summary(modelo)
speed <- rescale(cars$speed )
distancia <- rescale(cars$dist)
plot(speed, distancia)
lm(distancia ~ speed)
grad_cars<- grad_calc(speed,distancia)
iteraciones <- descenso(100000,c(0,0),0.005,grad_cars)
tail(iteraciones)
boston <- Boston
kable(head(boston))
pairs(boston)
names(boston )
dim(boston)
pairs(boston[,c(13,14)])
pairs(boston[,c(13,14)])
pairs(boston)
pairs(boston[,c(13,14)])
pairs(boston[,c(6,13,14)])
pairs(boston)
pairs(boston[,c(5,6,8,13,14)])
pairs(boston[,c(5,6,8,13,14)])
pairs(boston[,c(5,6,8,13,14)])
(modelo <- lm(medv ~ rm , data=boston))
summary(modelo)
modelo$fitted.values
plot(modelo$fitted.values, boston$medv)
ggplot(boston, aes(x = medv, y = rm)) + geom_point() +
geom_abline(slope = iteraciones[100,2] , intercept =iteraciones[100,1], col ='red') +
geom_abline(slope = modelo$coefficients[2], intercept = modelo$coefficients[1]
, col ='blue',linetype="dashed")+
theme_bw()f
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(tidyverse)
library(readr)
library(dplyr)
library(knitr)
library(scales)
z_0 <- 5
eta <- 0.4
>>>>>>> 95e33d06c7b1baeb53854478f76cd407677d78a7
descenso <- function(n, z_0, eta, h_deriv){
z <- matrix(0,n, length(z_0))
z[1, ] <- z_0
for(i in 1:(n-1)){
z[i+1, ] <- z[i, ] - eta * h_deriv(z[i, ])
}
z
}
<<<<<<< HEAD
iris_clas <- iris %>%
filter(Species != 'virginica')
table(iris_clas$Species)
length(iris_clas$Species)
iris_clas$spe <- ifelse(iris_clas$Species == "setosa",1,0)
iris_dev <- devianza_calc(iris_clas[1:4],iris_clas$spe)
iris_dev(c(0,0,0,0,0))
grad <- grad_calc(iris_clas[1:4],iris_clas$spe)
grad(c(0,0,0,0,0))
iter <- descenso(1000, c(0,0,0,0,0),.001,grad)
tail(iter)
modelo <- glm( spe ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data = iris_clas, family = binomial)
summary(modelo)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(scales)
h <- function(x) {exp(x)/(1+exp(x))}
devianza_calc <- function(x, y){
dev_fun <- function(beta){
x_beta <- as.matrix(cbind(1, x)) %*% beta
-2*sum(y*x_beta - log(1 + exp(x_beta)))
}
dev_fun
}
grad_calc <- function(x_ent, y_ent){
salida_grad <- function(beta){
p_beta <- h(as.matrix(cbind(1, x_ent)) %*% beta)
e <- y_ent - p_beta
grad_out <- -2*as.numeric(t(cbind(1,x_ent)) %*% e)
=======
grad_calc <- function(x_ent, y_ent){
salida_grad <- function(beta){
f_beta <- as.matrix(cbind(1, x_ent)) %*% beta
e <- y_ent - f_beta
grad_out <- -as.numeric(t(cbind(1, x_ent)) %*% e)
>>>>>>> 95e33d06c7b1baeb53854478f76cd407677d78a7
names(grad_out) <- c('Intercept', colnames(x_ent))
grad_out
}
salida_grad
}
<<<<<<< HEAD
descenso <- function(n, z_0, eta, h_deriv){
z <- matrix(0,n, length(z_0))
z[1, ] <- z_0
for(i in 1:(n-1)){
z[i+1, ] <- z[i, ] - eta * h_deriv(z[i, ])
}
z
}
iris_clas <- iris %>%
filter(Species != 'virginica')
table(iris_clas$Species)
length(iris_clas$Species)
iris_clas$spe <- ifelse(iris_clas$Species == "setosa",1,0)
iris_dev <- devianza_calc(iris_clas[1:4],iris_clas$spe)
iris_dev(c(0,0,0,0,0))
grad <- grad_calc(iris_clas[1:4],iris_clas$spe)
grad(c(0,0,0,0,0))
iter <- descenso(1000, c(0,0,0,0,0),.001,grad)
tail(iter)
modelo <- glm( spe ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data = iris_clas, family = binomial)
summary(modelo)
iris_clas$Sepal.Length <- scale(iris_clas$Sepal.Length)
iris_clas$Sepal.Width <- scale(iris_clas$Sepal.Width)
iris_clas$Petal.Length <- scale(iris_clas$Petal.Length)
iris_clas$Petal.Width <- scale(iris_clas$Petal.Width)
modelo <- glm( spe ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data = iris_clas, family = binomial)
summary(modelo)
iter <- descenso(1000, c(0,0,0,0,0),.001,grad)
tail(iter)
iris_clas$Sepal.Length <- scale(iris_clas$Sepal.Length)
iris_clas$Sepal.Width <- scale(iris_clas$Sepal.Width)
iris_clas$Petal.Length <- scale(iris_clas$Petal.Length)
iris_clas$Petal.Width <- scale(iris_clas$Petal.Width)
iris_dev <- devianza_calc(iris_clas[1:4],iris_clas$spe)
grad <- grad_calc(iris_clas[1:4],iris_clas$spe)
iter <- descenso(1000, c(0,0,0,0,0),.001,grad)
tail(iter)
modelo <- glm( spe ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data = iris_clas, family = binomial)
summary(modelo)
iter <- descenso(1000, c(0,0,0,0,0),.01,grad)
tail(iter)
iter <- descenso(1000, c(0,0,0,0,0),.01,grad)
iter <- descenso(10000, c(0,0,0,0,0),.01,grad)
iter <- descenso(10000, c(0,0,0,0,0),.01,grad)
tail(iter)
iter <- descenso(10000, c(0,0,0,0,0),.1,grad)
iter <- descenso(10000, c(0,0,0,0,0),.1,grad)
tail(iter)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(ggplot2)
h <- function(x){1/(1+exp(-x))}
# la devianza es la misma
devianza_calc <- function(x, y){
dev_fun <- function(beta){
p_beta <- h(as.matrix(cbind(1, x)) %*% beta)
-2*mean(y*log(p_beta) + (1-y)*log(1-p_beta))
}
dev_fun
}
# el cálculo del gradiente es el mismo, pero x_ent y y_ent serán diferentes
grad_calc <- function(x_ent, y_ent){
salida_grad <- function(beta){
p_beta <- h(as.matrix(cbind(1, x_ent)) %*% beta)
e <- y_ent - p_beta
grad_out <- -2*as.numeric(t(cbind(1,x_ent)) %*% e)/nrow(x_ent)
names(grad_out) <- c('Intercept', colnames(x_ent))
grad_out
}
salida_grad
}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(ggplot2)
h <- function(x){1/(1+exp(-x))}
# la devianza es la misma
devianza_calc <- function(x, y){
dev_fun <- function(beta){
p_beta <- h(as.matrix(cbind(1, x)) %*% beta)
-2*mean(y*log(p_beta) + (1-y)*log(1-p_beta))
}
dev_fun
}
# el cálculo del gradiente es el mismo, pero x_ent y y_ent serán diferentes
grad_calc <- function(x_ent, y_ent){
salida_grad <- function(beta){
p_beta <- h(as.matrix(cbind(1, x_ent)) %*% beta)
e <- y_ent - p_beta
grad_out <- -2*as.numeric(t(cbind(1,x_ent)) %*% e)/nrow(x_ent)
names(grad_out) <- c('Intercept', colnames(x_ent))
grad_out
}
salida_grad
}
descenso <- function(n, z_0, eta, h_deriv){
z <- matrix(0,n, length(z_0))
z[1, ] <- z_0
for(i in 1:(n-1)){
z[i+1, ] <- z[i, ] - eta * h_deriv(z[i, ])
}
z
}
# esta implementación es solo para este ejemplo:
descenso_estocástico <- function(n_epocas, z_0, eta, minilotes){
#minilotes es una lista
m <- length(minilotes)
z <- matrix(0, m*n_epocas, length(z_0))
z[1, ] <- z_0
for(i in 1:(m*n_epocas-1)){
k <- i %% m + 1
if(i %% m == 0){
#comenzar nueva época y reordenar minilotes al azar
minilotes <- minilotes[sample(1:m, m)]
}
h_deriv <- grad_calc(minilotes[[k]]$x, minilotes[[k]]$y)
z[i+1, ] <- z[i, ] - eta * h_deriv(z[i, ])
}
z
}
p_1 <- function(x){
ifelse(x < 30, 0.9, 0.9 - 0.007 * (x - 15))
}
set.seed(143)
sim_datos <- function(n){
x <- pmin(rexp(n, 1/30), 100)
probs <- p_1(x)
g <- rbinom(length(x), 1, probs)
# con dos variables de ruido:
dat <- data_frame(x_1 = (x - mean(x))/sd(x),
x_2 = rnorm(length(x),0,1),
x_3 = rnorm(length(x),0,1),
p_1 = probs, g )
dat %>% select(x_1, x_2, x_3, g)
}
dat_ent <- sim_datos(100)
dat_valid <- sim_datos(1000)
glm(g ~ x_1 + x_2+ x_3 , data = dat_ent, family = 'binomial') %>% coe
glm(g ~ x_1 + x_2+ x_3 , data = dat_ent, family = 'binomial') %>% coef
iteraciones_descenso <- descenso(300, rep(0,4), 0.8,
h_deriv = grad_calc(x_ent = as.matrix(dat_ent[,c('x_1','x_2','x_3'), drop =FALSE]),
y_ent=dat_ent$g)) %>%
data.frame %>% rename(beta_1 = X2, beta_2 = X3)
ggplot(iteraciones_descenso, aes(x=beta_1, y=beta_2)) + geom_point()
dat_ent$minilote <- rep(1:10, each=5)
split_ml <- split(dat_ent %>% sample_n(nrow(dat_ent)), dat_ent$minilote)
minilotes <- lapply(split_ml, function(dat_ml){
list(x = as.matrix(dat_ml[, c('x_1','x_2','x_3'), drop=FALSE]),
y = dat_ml$g)
})
length(minilotes)
iter_estocastico <- descenso_estocástico(30, rep(0, 4), 0.1, minilotes) %>%
data.frame %>% rename(beta_0 = X1, beta_1 = X2, beta_2 = X3)
ggplot(iteraciones_descenso, aes(x=beta_1, y=beta_2)) + geom_path() +
geom_point() +
geom_path(data = iter_estocastico, colour ='red', alpha=0.5) +
geom_point(data = iter_estocastico, colour ='red', alpha=0.5)
dev_ent <- devianza_calc(x = as.matrix(dat_ent[,c('x_1','x_2','x_3'), drop =FALSE]),
y=dat_ent$g)
dev_valid <- devianza_calc(x = as.matrix(dat_valid[,c('x_1','x_2','x_3'), drop =FALSE]),
y=dat_valid$g)
dat_dev <- data_frame(iteracion = 1:nrow(iteraciones_descenso)) %>%
mutate(descenso = apply(iteraciones_descenso, 1, dev_ent),
descenso_estocastico = apply(iter_estocastico, 1, dev_ent)) %>%
gather(algoritmo, dev_ent, -iteracion) %>% mutate(tipo ='entrenamiento')
dat_dev_valid <- data_frame(iteracion = 1:nrow(iteraciones_descenso)) %>%
mutate(descenso = apply(iteraciones_descenso, 1, dev_valid),
descenso_estocastico = apply(iter_estocastico, 1, dev_valid)) %>%
gather(algoritmo, dev_ent, -iteracion) %>% mutate(tipo ='validación')
dat_dev <- bind_rows(dat_dev, dat_dev_valid)
ggplot(filter(dat_dev, tipo=='entrenamiento'),
aes(x=iteracion, y=dev_ent, colour=algoritmo)) + geom_line() +
geom_point() + facet_wrap(~tipo)
=======
rss_calc <- function(datos){
# esta función recibe los datos (x,y) y devuelve
# una función f(betas) que calcula rss
y <- datos$lpsa
x <- datos$lcavol
fun_out <- function(beta){
y_hat <- beta[1] + beta[2]*x
e <- (y - y_hat)
rss <- sum(e^2)
0.5*rss
}
fun_out
}
prostata <- read_csv('datos/prostate.csv') %>% select(lcavol, lpsa, train)
kable(head(prostata), digits = 4)
prostata_entrena <- filter(prostata, train)
ggplot(prostata_entrena, aes(x = lcavol, y = lpsa)) + geom_point() + theme_bw()
rss_prostata <- rss_calc(prostata_entrena)
beta <- c(0,1.5)
rss_prostata(beta)
ggplot(prostata_entrena, aes(x = lcavol, y = lpsa)) + geom_point() +
geom_abline(slope = beta[2], intercept = beta[1], col ='red') + theme_bw()
beta <- c(1,1)
rss_prostata(beta)
ggplot(prostata_entrena, aes(x = lcavol, y = lpsa)) + geom_point() +
geom_abline(slope = beta[2], intercept = beta[1], col ='red') + theme_bw()
grad_prostata <- grad_calc(prostata_entrena[, 1, drop = FALSE], prostata_entrena$lpsa)
grad_prostata(c(0,1))
grad_prostata(c(1,1))
iteraciones <- descenso(100, c(0,0), 0.005, grad_prostata)
iteraciones
iteraciones[100,1]
(modelo <- lm(prostata_entrena$lpsa ~ prostata_entrena$lcavol))
summary(modelo)
ggplot(prostata_entrena, aes(x = lcavol, y = lpsa)) + geom_point() +
geom_abline(slope = iteraciones[100,2] , intercept =iteraciones[100,1], col ='red') +
geom_abline(slope = modelo$coefficients[2], intercept = modelo$coefficients[1]
, col ='blue',linetype="dashed")+
theme_bw()
plot(cars$dist,cars$speed)
cars <- cars
kable(head(cars))
modelo <-  lm(cars$dist ~ cars$speed)
modelo
summary(modelo)
speed <- rescale(cars$speed )
distancia <- rescale(cars$dist)
plot(speed, distancia)
lm(distancia ~ speed)
grad_cars<- grad_calc(speed,distancia)
iteraciones <- descenso(100000,c(0,0),0.005,grad_cars)
tail(iteraciones)
boston <- Boston
kable(head(boston))
pairs(boston)
pairs(boston[,c(5,6,8,13,14)])
(modelo <- lm(medv ~ rm , data=boston))
summary(modelo)
ggplot(boston, aes(x = medv, y = rm)) + geom_point() +
geom_abline(slope = modelo$coefficients[2], intercept = modelo$coefficients[1]
, col ='blue',linetype="dashed")+
theme_bw()f
ggplot(boston, aes(x = medv, y = rm)) + geom_point() +
geom_abline(slope = modelo$coefficients[2], intercept = modelo$coefficients[1]
, col ='blue',linetype="dashed")+
theme_bw()
(modelo <- lm(medv ~ rm , data=boston))
summary(modelo)
ggplot(boston, aes(x = medv, y = rm)) + geom_point() +
geom_abline(slope = modelo$coefficients[2], intercept = modelo$coefficients[1]
, col ='blue',linetype="dashed")+
theme_bw()
ggplot(boston, aes(x = rm, y = medv)) + geom_point() +
geom_abline(slope = modelo$coefficients[2], intercept = modelo$coefficients[1]
, col ='blue',linetype="dashed")+
theme_bw()
(modelo <- lm(medv ~ nox + rm + dis + lstat , data=boston))
summary(modelo)
ggplot(boston, aes(x = rm, y = modelo$fitted.values)) + geom_point() +
geom_abline(slope = 1, intercept = 0
, col ='blue',linetype="dashed")+
theme_bw()
ggplot(boston, aes(x = modelo$fitted.values, y = medv)) + geom_point() +
geom_abline(slope = 1, intercept = 0
, col ='blue',linetype="dashed")+
theme_bw()
cor(modelo$fitted.values,medv)
cor(modelo$fitted.values,boston$medv)
cor(modelo$fitted.values,boston$medv)^2
>>>>>>> 95e33d06c7b1baeb53854478f76cd407677d78a7
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(ggplot2)
h <- function(x){1/(1+exp(-x))}
# la devianza es la misma
devianza_calc <- function(x, y){
dev_fun <- function(beta){
p_beta <- h(as.matrix(cbind(1, x)) %*% beta)
-2*mean(y*log(p_beta) + (1-y)*log(1-p_beta))
}
dev_fun
}
# el cálculo del gradiente es el mismo, pero x_ent y y_ent serán diferentes
grad_calc <- function(x_ent, y_ent){
salida_grad <- function(beta){
p_beta <- h(as.matrix(cbind(1, x_ent)) %*% beta)
e <- y_ent - p_beta
grad_out <- -2*as.numeric(t(cbind(1,x_ent)) %*% e)/nrow(x_ent)
names(grad_out) <- c('Intercept', colnames(x_ent))
grad_out
}
salida_grad
}
descenso <- function(n, z_0, eta, h_deriv){
z <- matrix(0,n, length(z_0))
z[1, ] <- z_0
for(i in 1:(n-1)){
z[i+1, ] <- z[i, ] - eta * h_deriv(z[i, ])
}
z
}
# esta implementación es solo para este ejemplo:
descenso_estocástico <- function(n_epocas, z_0, eta, minilotes){
#minilotes es una lista
m <- length(minilotes)
z <- matrix(0, m*n_epocas, length(z_0))
z[1, ] <- z_0
for(i in 1:(m*n_epocas-1)){
k <- i %% m + 1
if(i %% m == 0){
#comenzar nueva época y reordenar minilotes al azar
minilotes <- minilotes[sample(1:m, m)]
}
h_deriv <- grad_calc(minilotes[[k]]$x, minilotes[[k]]$y)
z[i+1, ] <- z[i, ] - eta * h_deriv(z[i, ])
}
z
}
p_1 <- function(x){
ifelse(x < 30, 0.9, 0.9 - 0.007 * (x - 15))
}
set.seed(143)
sim_datos <- function(n){
x <- pmin(rexp(n, 1/30), 100)
probs <- p_1(x)
g <- rbinom(length(x), 1, probs)
# con dos variables de ruido:
dat <- data_frame(x_1 = (x - mean(x))/sd(x),
x_2 = rnorm(length(x),0,1),
x_3 = rnorm(length(x),0,1),
p_1 = probs, g )
dat %>% select(x_1, x_2, x_3, g)
}
dat_ent <- sim_datos(100)
dat_valid <- sim_datos(1000)
glm(g ~ x_1 + x_2+ x_3 , data = dat_ent, family = 'binomial') %>% coef
iteraciones_descenso <- descenso(300, rep(0,4), 0.8,
h_deriv = grad_calc(x_ent = as.matrix(dat_ent[,c('x_1','x_2','x_3'), drop =FALSE]),
y_ent=dat_ent$g)) %>%
data.frame %>% rename(beta_1 = X2, beta_2 = X3)
ggplot(iteraciones_descenso, aes(x=beta_1, y=beta_2)) + geom_point()
dat_ent$minilote <- rep(1:10, each=5)
split_ml <- split(dat_ent %>% sample_n(nrow(dat_ent)), dat_ent$minilote)
minilotes <- lapply(split_ml, function(dat_ml){
list(x = as.matrix(dat_ml[, c('x_1','x_2','x_3'), drop=FALSE]),
y = dat_ml$g)
})
length(minilotes)
iter_estocastico <- descenso_estocástico(30, rep(0, 4), 0.1, minilotes) %>%
data.frame %>% rename(beta_0 = X1, beta_1 = X2, beta_2 = X3)
ggplot(iteraciones_descenso, aes(x=beta_1, y=beta_2)) + geom_path() +
geom_point() +
geom_path(data = iter_estocastico, colour ='red', alpha=0.5) +
geom_point(data = iter_estocastico, colour ='red', alpha=0.5)
dev_ent <- devianza_calc(x = as.matrix(dat_ent[,c('x_1','x_2','x_3'), drop =FALSE]),
y=dat_ent$g)
dev_valid <- devianza_calc(x = as.matrix(dat_valid[,c('x_1','x_2','x_3'), drop =FALSE]),
y=dat_valid$g)
dat_dev <- data_frame(iteracion = 1:nrow(iteraciones_descenso)) %>%
mutate(descenso = apply(iteraciones_descenso, 1, dev_ent),
descenso_estocastico = apply(iter_estocastico, 1, dev_ent)) %>%
gather(algoritmo, dev_ent, -iteracion) %>% mutate(tipo ='entrenamiento')
dat_dev_valid <- data_frame(iteracion = 1:nrow(iteraciones_descenso)) %>%
mutate(descenso = apply(iteraciones_descenso, 1, dev_valid),
descenso_estocastico = apply(iter_estocastico, 1, dev_valid)) %>%
gather(algoritmo, dev_ent, -iteracion) %>% mutate(tipo ='validación')
dat_dev <- bind_rows(dat_dev, dat_dev_valid)
ggplot(filter(dat_dev, tipo=='entrenamiento'),
aes(x=iteracion, y=dev_ent, colour=algoritmo)) + geom_line() +
geom_point() + facet_wrap(~tipo)
