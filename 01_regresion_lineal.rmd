---
title: "01_regresion_lineal"
author: "Francisco Paz"
date: "2/9/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(tidyverse)
library(readr)
library(dplyr)
library(knitr)
library(scales)
library(gridExtra)
```

El propósito de este r-markdown es prácticar la regresión lineal usando R (más adelante veremos como funciona lo mismo pero en Python). 

Como vimos la clase pasada, existen distintas aproximaciones al problema, uno de ellos es el uso de mínimos cuadrados; en este caso veremos la implementación usando **Gradiente Descendente** y lo compararemos con la funcion _lm_ que viene en la paquetería básica de R.
```{r }
z_0 <- 5
eta <- 0.4
descenso <- function(n, z_0, eta, h_deriv){
  z <- matrix(0,n, length(z_0))
  z[1, ] <- z_0
  for(i in 1:(n-1)){
    z[i+1, ] <- z[i, ] - eta * h_deriv(z[i, ])
  }
  z
}
```


```{r funcion_gradiente}

grad_calc <- function(x_ent, y_ent){
  salida_grad <- function(beta){
    f_beta <- as.matrix(cbind(1, x_ent)) %*% beta
    e <- y_ent - f_beta
    grad_out <- -as.numeric(t(cbind(1, x_ent)) %*% e)
    names(grad_out) <- c('Intercept', colnames(x_ent))
    grad_out
  }
  salida_grad
}
```

```{r}
rss_calc <- function(datos){
  # esta función recibe los datos (x,y) y devuelve
  # una función f(betas) que calcula rss
  y <- datos$lpsa
  x <- datos$lcavol
  fun_out <- function(beta){
    y_hat <- beta[1] + beta[2]*x
    e <- (y - y_hat)
    rss <- sum(e^2)
    0.5*rss
  }
  fun_out
}
```

```{r}
prostata <- read_csv('datos/prostate.csv') %>% select(lcavol, lpsa, train)
kable(head(prostata), digits = 4)
```


```{r}
prostata_entrena <- filter(prostata, train)
ggplot(prostata_entrena, aes(x = lcavol, y = lpsa)) + geom_point() + theme_bw()
```

```{r}
rss_prostata <- rss_calc(prostata_entrena)
beta <- c(0,1.5)
rss_prostata(beta)
```

```{r}
ggplot(prostata_entrena, aes(x = lcavol, y = lpsa)) + geom_point() +
  geom_abline(slope = beta[2], intercept = beta[1], col ='red') + theme_bw()
```


```{r}
beta <- c(1,1)
rss_prostata(beta)
```

```{r}
ggplot(prostata_entrena, aes(x = lcavol, y = lpsa)) + geom_point() +
  geom_abline(slope = beta[2], intercept = beta[1], col ='red') + theme_bw()
``` 

```{r}
grad_prostata <- grad_calc(prostata_entrena[, 1, drop = FALSE], prostata_entrena$lpsa)
grad_prostata(c(0,1))
```

```{r}
grad_prostata(c(1,1))
```

```{r}
iteraciones <- descenso(100, c(0,0), 0.005, grad_prostata)
iteraciones
```

```{r}
iteraciones[100,1]
``` 


 
Finalmente comparamos con la función _lm_ 

```{r}
(modelo <- lm(prostata_entrena$lpsa ~ prostata_entrena$lcavol))
```

```{r}
summary(modelo)
```

```{r}
ggplot(prostata_entrena, aes(x = lcavol, y = lpsa)) + geom_point() +
  geom_abline(slope = iteraciones[100,2] , intercept =iteraciones[100,1], col ='red') +
  geom_abline(slope = modelo$coefficients[2], intercept = modelo$coefficients[1]
              , col ='blue',linetype="dashed")+ 
  theme_bw()
```
Vimos un ejemplo de regresión lineal **simple** usando **gradiente en descenso** y con la función **lm** (del paquete básico de R). ¿Puedes pensar en algún problema causado por el uso de gradiente descendente?

+ Es bastante recomendable normalizar las entradas, para evitar tener problemas al momento de movernos.

A continuación se presentan algunos ejercicios para consolidar el uso de **lm** para regresión.


1. Utilizando la base de datos _cars_ realiza los siguientes ejercicios.
  + Un análisis exploratorio (al menos un scatter plot) de la base
  
```{r}
plot(cars$dist,cars$speed)
```
  
  + Realiza la regresión utilizando la función _lm_ y la función de gradiente que construimos.
```{r}
cars <- cars
kable(head(cars))
```

```{r}
modelo <-  lm(cars$dist ~ cars$speed)
modelo
```

```{r}
summary(modelo)
```

```{r}
grad_cars<- grad_calc(cars$speed,cars$dist)
iteraciones <- descenso(100,c(0,0),0.005,grad_cars)
iteraciones
```

Como vimo en clase, la escala de las variables es muy importante para la solución del problema. Notemos que se reescalan las variables y el algoritmos ya converge! (a diferencia del problema que vimos en clase).

```{r}
speed <- rescale(cars$speed )
distancia <- rescale(cars$dist)
plot(speed, distancia)
lm(distancia ~ speed)
```

Puedes tratar de cambiar el paso de aprendizaje  $\eta = 0.005$ y ver que ocurre.

```{r}
grad_cars<- grad_calc(speed,distancia)
iteraciones <- descenso(100000,c(0,0),0.005,grad_cars)
tail(iteraciones)
```
2. La base de datos _Boston_ (también incluida en r básico ) contiene información acerca de casa en Boston (adivinaron). La idea sería construir una regresión lineal simple y una múltiple para calcular la variable __medv__.

  + Selecciona una variable que creeas esta altamente relacionada con el precio (si necesitas saber el significado de cada variable, puedes usar)
```{r, eval=FALSE}
?Boston  
```

  + Toma al menos 3 variables y realiza una regresión lineal múltiple.


Aquí cargamos la vase y mostramos las primeras observaciones de la misma. 

```{r}
boston <- Boston
kable(head(boston))
````


```{r}
pairs(boston)
```

```{r}
pairs(boston[,c(5,6,8,13,14)])
```

```{r}
(modelo1 <- lm(medv ~ rm , data=boston))
```

```{r}
summary(modelo1)
```

```{r}
ggplot(boston, aes(x = rm, y = medv)) + geom_point() +
  geom_abline(slope = modelo1$coefficients[2], intercept = modelo1$coefficients[1]
              , col ='blue',linetype="dashed")+ 
  theme_bw()
```

Para el caso de regrsión multiple solo agregaremos las variables que nos interesan al comando del modelo. 

```{r}
(modelo <- lm(medv ~ nox + rm + dis + lstat , data=boston))
```

```{r}
summary(modelo)
```


¿Se te ocurre alguna forma de ver el ajuste como en el caso lineal?

La idea de la siguiente gráfica es simple, queremos que los valores **ajustados** vs los valores **reales** queden dentro de la linea identidad. 

```{r}
g1 <- ggplot(boston, aes(x = modelo$fitted.values, y = medv)) + geom_point() +
  geom_abline(slope = 1, intercept = 0
              , col ='blue',linetype="dashed")+ 
  theme_bw()

g1
```

Por ejemplo, para la regresión simple se vería así la gráfica:

```{r}
g0 <- ggplot(boston, aes(x = modelo1$fitted.values, y = medv)) + geom_point() +
  geom_abline(slope = 1, intercept = 0
              , col ='blue',linetype="dashed")+ 
  theme_bw()

g0
```


¿cómo harías para agregar todas las variables al análisis?

```{r}
(modelo <- lm(medv ~ ., data = boston))
```

```{r}
summary(modelo)
```

¿Notas algun error?

```{r}
boston$chas <- as.factor(boston$chas)
```


```{r}
(modelo <- lm(medv ~ ., data = boston))
```

```{r}
summary(modelo)
```


```{r}
g2 <- ggplot(boston, aes(x = modelo$fitted.values, y = medv)) + geom_point() +
  geom_abline(slope = 1, intercept = 0
              , col ='blue',linetype="dashed")+ 
  theme_bw()

g2
```

```{r}
#library(gridExtra)
grid.arrange(g0,g1,g2,nrow=1)
```




